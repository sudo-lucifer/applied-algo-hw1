% You should title the file with a .tex extension (hw1.tex, for example)
\documentclass[a4paper, 12pt]{report}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{fancyhdr}
\usepackage{minted}
\usepackage{amsthm}        
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,      
    urlcolor=cyan,
    pdfpagemode=FullScreen,
}

\renewcommand\qedsymbol{$\blacksquare$}


\graphicspath{ {./pictures} }

\usepackage[margin=1in]{geometry}

\newcommand{\question}[2] {\vspace{.25in} \hrule\vspace{0.5em}
\noindent{\bf #1: #2} \vspace{0.5em}
\hrule \vspace{.10in}}
\renewcommand{\part}[1] {\vspace{.10in} {\bf (#1)}}

\newcommand{\myname}{Krittin Nisunarat}
\newcommand{\myhwnum}{1}
\newcommand{\mySubject}{ICCS315 Applied Algorithms}

\setlength{\parindent}{20pt}
\setlength{\parskip}{5pt plus 1pt}
 
\pagestyle{fancyplain}
\lhead{\fancyplain{}{\textbf{HW} \myhwnum}}      % Note the different brackets!
\rhead{\fancyplain{}{\myname}}
\chead{\fancyplain{}{\mySubject}}

\title{
	{\includegraphics[width=80mm,scale=0.5]{MUIC_Logo_Eng_Center.png}} \\
	{\textbf{\mySubject}}\\
	{\large Assignment \myhwnum \ Report}\\
}
\author{
	{\textbf{Written by}} \\ 
	{Krittin Nisunarat 6280782}
}
\date{30 Jan 2022}
\thispagestyle{plain}


\begin{document}
\maketitle
\tableofcontents

\chapter{Resizeable Arrays}

Traditionally, resizable array has a high amortized cost depending on $\alpha$ for expansion and shrinking. HAT array theoretically fares
with a constant amortized cost on expansion and shrinking. Our goal is to perform teh benchmarking of the actual implementation of 
traditionaly resizable array and Sitarski's HAT array.

\section{Set up}

Both of data structure implementations are written in C++ which runs on ubuntu server. 
The actual implement is in \href{https://github.com/sudo-lucifer/applied-algo-hw1/tree/main/hat_array}{Github}.

\section{Append latency}

In this section, the cost of appending one key to the array mainly comes from reading memory. 
On tradditional resizable array, pushing new key to the back costs approximatetly 47 cycles
from reading memory pointer which needs to be conducted at most the entire array. On the other hand,
Sitarski's HAT array takes less cycle per apeend operation which is 26 cycles since the entire array is separated 
into $b$ size where $b$ where $b = 2^k$.

\begin{figure}[h]
        \centering
        \includegraphics[width=0.4\textwidth,scale=0.2]{append_latency_output.png}
        \caption{\label{fig:append-latency} Benchmark of each append costs}
\end{figure}

Figure \ref{fig:append-latency} shows the cost of each append out of 50 append operations.
Tradditional resizable array shows up to stike CPU cycle more on each operation since the expansion
happens more often than HAT array. However, HAT array costs significantly higher cycle because the 
resizing needs to be done in both levels combine with copying data over.

In conclusion, traditional resizable array takes more cycle to access memory and append new day than
HAT array. On the expansion, tradditional resizable array is better than HAT array.

\section{Access latency}

Getting element by index in HAT array is tricky in the way that we need to calculate the block index and 
the index of element in particular block. This seems to take more cycle than traditional resizable array. 
Surprisingly, HAT array uses slightly less cycle then resizable array.
Benchmark is finding the average (out of 100,000 times) CPU count on each data structure element access. The result
is that HAT array takes 33 cycles on average while traditional resizable array takes 38 cycles. Therefore,
HAT array access element faster by 8 percents approximately.

\section{Scan throughput}

Continue from section 1.2, this section will illustrate the total latency of accesing/reading all elements
in array. Let's start with both arrays have 500 elements. HAT array costs 16800 cycles in total while 
traditional resizable array costs 18918 cycles on reading.

\begin{figure}[h]
        \centering
        \includegraphics[width=0.4\textwidth,scale=0.2]{scan_output.png}
        \caption{\label{fig:scan-latency} Benchmark of scanning through array}
\end{figure}

As we can see in figure \ref{fig:scan-latency}, HAT array is nearly stable on reading after 300 elements.
Resizeable array, on the other side, have a few operation that strike CPU cycle which reaches over 45 cycles.
In conclusion, HAT array still wins in this competition that it scans faster by 13 percents.

\section{Overall throughput}

In section 1.1, I have shown append latency which conclude to have different benefits.
For this benchmark, I will test inserting 50 elements to both tradditional resizable array and
HAT array. As a consequence, resizable array takes 1153 cycles on average while HAT array takes
1114 cycles on average. Even though the expansion is costly in HAT array, overall throughput
of it is around 5 percents faster than resizable array.


\chapter{Skip lists}

\section{One directional skip list vs. sorted linked list}

Sorted linked list theoretically takes $O(n)$ complexity for insertion and scan throughput. 
Skip list comes to take place in theory land that it improve time complexity of those 2 operations.
For insertion, skip list takes $O(\log_2 n)$ with whp. The same thing goes with scan throughput since
we guarantee with high probability that every access costs $\Theta(log_2 n)$ where $n$ is the number of elements.
Let's see the real breanchmark using c++ language on insertion and scan throughput.


\subsection{Append latency}
Overall, average cycle count for 10,000 insertions in skip list is 688 cycles while 
linked list takes 44,836 cycles. This means that skip list insertion is 65x faster than linked list.
The following picture shows the cpu cycle on each insert operation on both lists.

\begin{figure}[h]
        \centering
        \includegraphics[width=0.5\textwidth,scale=0.5]{skip_list_insert.png}
        \caption{\label{fig:skip_list_insert} CPU cycle per insert operation}
\end{figure}

As you can see in figure \ref{fig:skip_list_insert}, skip list insertion is nearly stable at between 400 and 500 cycles with a few strike.
On the other hand, since linked list takes $O(n)$ to insert an element, the cycle count is higher as the size of array is increased over each operation.
The result also implies that linked list has a better performance on a small $n$ size while skip list clearly wins the benchmark in a larger scale.

\subsection{Scan throughput}
Overall scan performance goes the same way as the result in section 2.1.1. Skip list takes only 186 cycles on average to search element with 10,000 elements in total
while linked list takes 98,651 cycles to search one element. In total, with 10,000 elements, skip list takes 1,860,489 cycles to scan all elements in list while linked list
uses up to 985,616,508 cycles. In this competition, skip list clearly wins as it is 530x faster.

\begin{figure}[h]
        \centering
        \includegraphics[width=0.5\textwidth,scale=0.5]{skip_list_scan.png}
        \caption{\label{fig:skip_list_scan} CPU cycle per search operation}
\end{figure}

\section{Bi-directional search}

Searching in one directional skip list takes $O(\log_2 n)$ where $n$ is the amount of element in array since we start from the top layer.
Let's improve the running time of search algorithm by allowing it to go up and down. In this way, we look for the layer to start searching, 
instead of alway start from the highest level. In this way, the search will take $O(log_2 d)$ where d is the amount of elements smaller than the given key
because the highest level we can go is $O(log_2 d)$ with high probability. This also means that traversing up and down at $\log_2 d$ level also takes 
$2\log_2 d$ to the desired key. In total, the search takes $2\log_2 d + 2\log_2 d = O(\log_2 d)$. Let's take a look at pseudo code.

\begin{minted}{c}
search(int element) {
        let x -> header;
        let start_node -> header;
        let layer -> 0;
        -- find start node starting from bottom layer (traverse up)
        for i = 0 -> max level so far {
                if x->forward[i].key < element:
                        start_node = x->forward[i].key;
                        layer = i;
        }
        -- traverse down to desired key
        for i = layer -> 0 (inclusive) {
                while start_node->forward[i] < element:
                start_node = start_node->forward[i];
        };
        -- check final key
        if start_node != NULL and start_node->key == element {
                return start_node.key
        }
        return NULL;
}
\end{minted}

\subsection{Benchmark}

Let's see the performance difference between one-directional skip list and bi-direction skip list search algorithm.
The benchmark is to measure scan throughput of 1,000 sizes with 20 max levels. The result is the following.

\begin{figure}[h]
        \centering
        \includegraphics[width=0.5\textwidth,scale=0.5]{skip_list_v2_scan.png}
        \caption{\label{fig:skip_list_v2_scan} CPU cycle per search operation}
\end{figure}

The performance is slightly different that bi-directional search takes 436 cycles on average while one-directional search takes
451 cycles. In conclusion, bi-directional skip list search is faster by 4\%.



\chapter{(a,b) tree}

\section{Multiple keys insertion.}

Starting with an empty tree, we want to insert the following keys:
\begin{center}
        733, 703, 608, 846, 309, 269, 55, 745, 548, 449, 513, 210, 795, 656, 262
\end{center}

The result of \emph{(2,3) tree} is

\begin{figure}[h]
        \centering
        \includegraphics[width=0.8\textwidth,scale=0.5]{tree_insertion.jpeg}
\end{figure}

\section{Key deletion}

\noindent Suppose that we want to delete 309 in \emph{(2,3) tree} (Firgure \ref{fig:tree-deletion-1}), it falls into case 2 which 
we need to merge from sibling (Figure \ref{fig:tree-deletion-2}).

\begin{figure}[h]
        \centering
        \includegraphics[width=0.7\textwidth,scale=0.5]{tree_deletion_1.jpeg}
        \caption{\label{fig:tree-deletion-1} Unbalanced tree without key 309}
\end{figure}

\begin{figure}[h]
        \centering
        \includegraphics[width=0.8\textwidth,scale=0.5]{tree_deletion_2.jpeg}
        \caption{\label{fig:tree-deletion-2} Tree after merging}
\end{figure}

However, the merged tree in figure \ref{fig:tree-deletion-2} is not balance as the second layer node has 3 keys. So, we need to project 449 up to root.

\begin{figure}[h]
        \centering
        \includegraphics[width=0.8\textwidth,scale=0.5]{tree_deletion_3.jpeg}
        \caption{\label{fig:tree-deletion-3} Final tree}
\end{figure}


As you can see in Figure \ref{fig:tree-deletion-3}, the replacements are 449 which is the result from merging 
non-root tree in figure \ref{fig:tree-deletion-2}.
\begin{align*}
        \alpha_{\text{me}} &= a - 1 \\
        \alpha_{\text{sib}} &= a \\
        \alpha_{\text{sib}} + \alpha_{\text{me}} &= 2a - 1 \leq b
\end{align*} 


\chapter{B-tree speed}
\end{document}
